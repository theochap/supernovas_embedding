# -*- coding: utf-8 -*-
"""WP-SN3_realData.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SZ2DqMrNq47gthwfMlhB1qz39kvwwE12

## **WP-SN-3**

** AUTHOR : THEODORE CHAPUIS-CHKAIBAN **

**Organization of the work**

Our job was to finalize SN work and with the CMB team to get an approximation of the possibles cosmological constants. First, we had to fit the lightcurves of supernovae in order to obtain the distance modulus for each of them (taking into account redshift and brighter-slower effect). Then we built a graph of distance modulus as a function of redshift, with a log scale for the x-axis, and compared it to canonical models. Finally we were able to approach the values of cosmological constant we were looking for.

### **I. Importation of data and packages**

The trick here is to be aware that the supernova are not sort by their number in the file.
"""

#### Reading file from Google Drive
!pip install PyDrive
import os
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

##### For Data-LightCurves (WP-SN-3):
download = drive.CreateFile({'id': '1Ih8AQsMCmSESWytcztyJKGVvnP_-VAsC'})
download.GetContentFile('Data-LightCurves.tgz')
!tar zxvf Data-LightCurves.tgz

download_mag = drive.CreateFile({'id': '158CYzUijoICCIGfSc2YjAg_tJDcCAsa8'})
download_mag.GetContentFile('magnitudes.txt')
!ls -lrt

download_red = drive.CreateFile({'id': '1m7-JZeKGqn_kBPYBCOgkAPWxOS4jFcmV'})
download_red.GetContentFile('Data-Search-SNe.xlsx')
!ls -lrt

# Commented out IPython magic to ensure Python compatibility.
#### Some usual imports
from pylab import *
from matplotlib import rc
# %matplotlib inline
rc('figure',figsize=(17,10))
rc('font',size=12)
rc('text',usetex=False)
import numpy as np
import matplotlib.pyplot as plt
import sys
import os
import logging
import cmath as cm
import math
import random
import scipy
from scipy.stats import norm
from astropy.io import fits
from astropy.cosmology import WMAP9 as cosmo
import pandas as pd
pd.options.display.float_format = '{:,.2f}'.format
import glob

"""### **II. Observation of provided data**

Let's plot the average lightcurve to remind us its shape.
"""

t0, mag0 = np.loadtxt('Data-LightCurves/EI2019-Data-LightCurves-SN-SNI-Average_LightCurve.txt').T


print(t0, mag0)

plot(t0,-mag0)
xlabel('Time in Supernova referential frame')
ylabel(' - Absolute magnitude')

"""This does not account for the brighter-slower empirical law that states that the magnitude reduces by 1.52 times the difference of the time stretch factor w.r.t one.

With the following small piece of code, we create a class to account for the stretch factor and correct for the brighter-slower effect. The plot is focused on the maximum and take a large gap of stretch factor to have a look on its role.
"""

class SNIaLightCurve:
    def __init__(self, data):
        self.data = data
        self.t0 = 0.  
    def __call__(self,t,s, z=0):
      return np.interp(t, (self.data[:,0])*s*(z+1), self.data[:,1], left=0,right=0) - 1.52*(s-1)   

      #We modified the class to take the redshift effect into account
      
lc = SNIaLightCurve(np.array([t0,mag0]).T)

svals = np.linspace(0.7, 2.1, 7)

ttt = np.linspace(-5, 20, 100)

for i in range(len(svals)):
  plot(ttt, -lc(ttt,svals[i], 0), label='s={0:5.2f}'.format(svals[i]))

xlabel('Time in Supernova referential frame')
ylabel(' - Absolute magnitude') 
legend()

"""Let's now work on untreated data of lightcurve of supernova, we will extract their apparent magnitude (with the error linked) depending of the time in the Supernova referantial frame and the redshift linked to each Supernova."""

#Affichage des données brutes : 

from os import walk
mag = {}
t = {}
err = {}
for (repertoire, sousRepertoires, fichiers) in walk('Data-LightCurves'):
    for fichier in fichiers:
      if fichier == 'EI2019-Data-LightCurves-SN-SNI-Average_LightCurve.txt' or fichier == 'EI2019-Data-LightCurves-SN-Redshifts.txt':
        continue

      name = fichier
      bla = str.split(name,'_')
      parts = str.split(bla[0], '-')
      number = int(parts[-1])

      mag[number] = np.loadtxt(repertoire + '/' + fichier).T[1]
      err[number] = np.loadtxt(repertoire + '/' + fichier).T[2]
      t[number] = np.loadtxt(repertoire + '/' + fichier).T[0]

subplot(4,1,1)
ylabel('Apparent magnitude')
  
subplot(4,1,2)
ylabel('Error on magnitude')

xlabel('Time in Supernova referential frame')

subplot(4,1,3)
ylabel('Magnitude + error')

xlabel('Time in referential frame')

subplot(4,1,4)
xlabel('Supernova number')
ylabel('Redshift')



for i in range(0,50):
  subplot(4,1,1)
  plot(t[i], -mag[i])
  subplot(4,1,2)
  plot(t[i], err[i])  

  subplot(4,1,3)
  errorbar(t[i], -mag[i],yerr=err[i])

z = np.loadtxt('Data-LightCurves/EI2019-Data-LightCurves-SN-Redshifts.txt')
subplot(4,1,4)
plot(z, 'ro')


print(z[0] )

"""To have a look on the uncertainty impact, let's plot the lightcurve of the first supernova in the file. As we can see in the second plot before, the uncertainty is growing with time."""

t_ini, mag_ini, e = np.loadtxt('Data-LightCurves/EI2019-Data-LightCurves-SN-0_lightcurve.txt').T

plt.plot(t_ini, -mag_ini)
plt.errorbar(t_ini, -mag_ini, yerr = e, fmt = 'none', ecolor ='black', capsize = 5)
print(e)

"""### **III. Optimization of the features of the lightcurve model to the Supernova**

First of all we import the package Minuit that will help us for the optimization.
"""

# Commented out IPython magic to ensure Python compatibility.
#Code Minuit

from pylab import *
import numpy as np
# %matplotlib inline
from matplotlib import rc
rc('figure',figsize=(10,5))
rc('font',size=12)
rc('text',usetex=False)
!pip install pymc
!pip install iminuit
!rm -rf cosmolib.py*
!wget --no-check-certificate https://raw.githubusercontent.com/jchamilton75/CS-ST4-Cosmo-2020/master/cosmolib.py
import cosmolib as cs

"""Now we have to find the pamaters (call pars) that fit the best with the model, Minuit will return the values and the uncertainties (covariance matrix). Doing the procedure on the fifth first supernova let us have an idea of the order of magnitude."""

#Fitting avec Minuit

result=150*[0]
def fun_to_fit(x, pars):
    values = pars[0] - lc(x - pars[2], pars[1] , z[i] )
    return values

for i in range(0, 150):
  try:
    guess = np.array([-40., 1., 0.]) #Initial guess that roughly follows the mean values of the data parameters
    result[i] = (cs.do_minuit(t[i], - mag[i], err[i], guess, functname=fun_to_fit))
    print(result[i])
  except:
    continue

print('Fin des résultats')

"""Thus we the feature we obtain we could plot the j first supernova data (by default j = 50) with the correction we got (brighter-slower, redshift) :

We may note, since minuit is very sensitive to initial parameters, that some curves don't fit quite well : this is due to a misadjustment of the initial guess for the 3rd variable (time at maximum)
"""

#Affichage des courbes

j = 150

trash_values = []

plt.figure(figsize=(15,10))
plt.xlabel('Time in our reference frame (in days)')
plt.ylabel('- Absolute magnitude')

for i in range(0, 150):

  if type(result[i]) == int or max((t[i] - result[i][1][2])/((z[i]+1)*result[i][1][1])) > 30 :
    trash_values.append(i)
    continue
  
  path =r"Data-LightCurves/EI2019-Data-LightCurves-SN-"+str(i)+"_lightcurve.txt"
  tp, magp, errp = np.loadtxt(path).T
  plot(t[i], -lc(t[i], 1) )
  
  errorbar((t[i] - result[i][1][2])/((z[i]+1)*result[i][1][1]), -mag[i]-result[i][1][0] - 1.52*(result[i][1][1]-1),yerr = errp, fmt='.', markersize=5, elinewidth=1) #taille des points modifiable dans markersize pour la clarté

"""Even if the fitting seems satisfying, our precedent method didn't provide convenient results, thus we made the optimization with scipy.

Scipy provide tools for non-linear optimization that relies less on an initial guess, which was the main issue we experienced with Minuit.
"""

#Approximation avec scipy

import matplotlib.pyplot as plt
from scipy.optimize import curve_fit

result_opt=[]
result_min=[]
error = []

j = 150 #Number of supernovas to fit

def fun_to_fit(x, par0, par1, par2):
  return (par0 - lc(x-par2, par1, z[i]) ) #Par0 : distance modulus, par1 : scale factor, par2 : time at maximum

for i in range(0, j):
  
  result_opt.append(curve_fit(fun_to_fit, t[i], -mag[i]))
  
  print("Supernova n°{}".format(i))
  print("Parameters value (distance modulus, scale factor, time at maximum) : ")
  print(result_opt[i][0])
  print("Covariance matrix with parameters in the same order : ")
  print(result_opt[i][1])

"""Once the fitting is done, we can plot the corrected lightcurves.

The result is quite satisfying : we have corrected the initial condition sensitivity issues.
"""

for i in range(0, 150):
  t_corr = (t[i]-result_opt[i][0][2])/((z[i]+1)*(result_opt[i][0][1])) #Time corrected for redshift, scale and time at maximum effects of the supernovae
  mag_corr = -mag[i]-result_opt[i][0][0] - 1.52*(result_opt[i][0][1]-1) #Magnitude corrected for redshift, scale and time at maximum effects of the supernovae

  #We inverse here the relation to plot the apparent curves on the absolute mean curve

  errorbar(t_corr , mag_corr, yerr = errp, fmt='.', markersize=5, elinewidth=1)

  #We also plot the curve of the average magnitude

  plot(t[i], -lc(t[i], 1) )

"""To evaluate the bias on the global fitting we plot the mean of the values of the curves of the supernovas on the average lightcurve.

We also plot the bias on our measures.
"""

#Here we calculate the mean of the values of the curves of the supernovas

delta=200 #Number of intervals

dt = linspace(-10, 30, delta) #Time interval for calculation

bin = {} #Contains the mean value at a time t

for i in range(0, j):
  for j in range(0, len(t[i])):
    dti = floor(((t[i][j]-result_opt[i][0][2])/((z[i]+1)*(result_opt[i][0][1])) + 10)*delta/40) #We get the index of dt corresponding to the corrected time on the curve

    if dti not in bin:
      bin[dti] = [-mag[i][j]-result_opt[i][0][0] - 1.52*(result_opt[i][0][1]-1), 1] #We create a bin if we don't know any previous value at a time dti
      
    else:
      bin[dti][0] = (-mag[i][j]-result_opt[i][0][0] - 1.52*(result_opt[i][0][1]-1) + bin[dti][0]*bin[dti][1])/(1+bin[dti][1]) #We update the previous value and calculate the mean
      bin[dti][1] += 1 #We update the number of data that is used for calculating bin[dti] (used to make a mean)

liste = delta*[0]

for i in range(0,delta): #We complete the time gap for intermediate values that weren't listed previously
  if i in bin:
    liste[i] = bin[i][0]
  else:
    if i != 0:
      liste[i] = liste[i-1]
    else:
      liste[i] = 0


subplot(2,1,1)  
plot(dt[7:-20], array(liste[7:-20])) #To plot a continuous line of the results
plot(t[0], -lc(t[0], 1) )


subplot(2,1,2)
plt.scatter(dt[7:-20], array(liste[7:-20]), c = 'red', s=0.8) #To plot a scattered line of the results
plot(t[0], -lc(t[0], 1) )


bias = 0
for k in range(0, len(dt)-20): #Calculate the bias on the result
  if liste[k] == 0 or lc(dt[k],1) == 0:
    continue

  bias += (liste[k]+lc(dt[k], 1))
  

print("Biais sur mesure : " + str(bias/(len(dt) - 43)))

"""### **IV. Curve of distance modulus as a fonction of redshift**

Now we have obtained satisfactory results, let's put them into a list and an array to process them.
"""

#Liste Redshift,Moduli
modulist=150*[0]
for i in range(0, 150):
  
  modulist[i]=[result_opt[i][0][0], z[i]] #List that contains distance modulus (first index) and redshift (second index)

print("Distance modulus, redshift :")
print(modulist)

#Array Redshift
z=[]
for i in range(150):
  z.append(modulist[i][1])
zsn=asarray(z)

#Array Moduli
mu=[]
for i in range(150):
  mu.append(-modulist[i][0])
musn=asarray(mu)

#Array Error
dmu=[]
for i in range(150):
  dmu.append(sqrt(result_opt[i][1][0][0]))
dmusn=asarray(dmu)

"""And finally we plot into a graph the evolution of distance modulus as a function of redshift with a log scale."""

#Graph

errorbar(zsn, musn, dmusn, fmt='k.')
xscale('log')
xlabel('Redshift')
ylabel('Distance Modulus')

"""This result is satisfactory and seems quite relevant in terms of uncertainties

We can compare it to canonical models (namely the flat one, the open one, and the lcdm one) to question the relevance of our results.
"""

cosmo_lcdm = {'h':0.7, 'omega_M_0':0.3, 'omega_lambda_0':0.7, 'w0':-1.}
cosmo_flat = {'h':0.7, 'omega_M_0':1.0, 'omega_lambda_0':0.0, 'w0':-1.}
cosmo_open = {'h':0.7, 'omega_M_0':0.3, 'omega_lambda_0':0.0, 'w0':-1.}

### Calling cs.mus1a()
#mu_flat = 5*np.log10(cs.lumdist(zsn, cosmo_flat)*1e6)-5
mu_lcdm = cs.musn1a(zsn, cosmo_lcdm)
mu_flat = cs.musn1a(zsn, cosmo_flat)
mu_open = cs.musn1a(zsn, cosmo_open)

### Chi2 calculation
chi2_flat = np.sum( (musn - mu_flat)**2/dmusn**2)
chi2_lcdm = np.sum( (musn - mu_lcdm)**2/dmusn**2)
chi2_open = np.sum( (musn - mu_open)**2/dmusn**2)

errorbar(zsn, musn, yerr=dmusn, fmt='k.', zorder=1)

plot(zsn, mu_flat, lw=1, color='blue',
     label='Flat matter only: $\chi^2$ = {0:5.1f} - ndf = {1:}'.format(chi2_flat,len(zsn)), zorder=2)
plot(zsn, mu_lcdm, lw=1, color='green',
     label='$\Lambda$CDM: $\chi^2$ = {0:5.1f} - ndf = {1:}'.format(chi2_lcdm,len(zsn)), zorder=3)
plot(zsn, mu_open, lw=1, color='red',
     label='Open: $\chi^2$ = {0:5.1f} - ndf = {1:}'.format(chi2_open,len(zsn)), zorder=4)

legend()
xscale('log')
xlim(2e-1,1.5)
ylim(39,45.5)
xlabel('Redshift')
ylabel('Distance Modulus')

"""### **V. Determination of cosmological constants**

Thanks to extracted results from data, we are able to approach the real values of the cosmological parameters we were looking for.
"""

def newmusn(x, pars):
  cosmo = {'h':pars[0], 'omega_M_0':pars[1],
           'omega_lambda_0':pars[2], 'w0':pars[3]}
  val = cs.musn1a(x, cosmo)
  val[isnan(val)] = -1e30
  return val

data = cs.Data(xvals=np.array(zsn), yvals=np.array(musn), errors=np.array(dmusn), model=newmusn)

chainomol = cs.run_mcmc(data, allvariables=['h','om', 'ol','w'],
                        fitvariables=['om','ol'],
                        fidvalues = np.array([0.7, 0.3, 0.7, -1]))

sm=3
cs.matrixplot(chainomol, ['om', 'ol'], 'blue', sm,
              limits = [[0,1], [0,1.]],
              labels=['$\Omega_m$', '$\Omega_\Lambda$'])
subplot(2,2,3)
plot(linspace(0,1,10),1-linspace(0,1,10),'k--')

"""### **VI. Using SN1 & SN2 Data**

Now we work with the data given by SN1 & SN2 groups : SN1 for the redshift for each supernova and SN2 for the magnitudes at different epoch for each supernova. However our result won't be convenient since SN2 didn't provide the error magnitude.
"""

magSN2 = []
tSN2 = []

supernova_num = -1

real_mag = open('magnitudes.txt', 'r')
for line in real_mag:
  s=line.strip("\n\r")
  l=s.split(";")

  if l[1] == 'Supernova_Number':
    continue

  if supernova_num != l[1]:
    supernova_num = l[1]
    magSN2.append(magi)
    tSN2.append(timei)

    timei = []
    magi = []
    timei.append(float(l[0]))
    magi.append(float(l[2]))
  
  else :
    timei.append(float(l[0]))
    magi.append(float(l[2]))
  


print(magSN2)

redshift = []
df = []
redshift = []
for i in range(0,5):
  df.append(pd.read_excel (r'Data-Search-SNe.xlsx', sheet_name='Data-Search-Field-{}'.format(i)) )
  for x in df[i].itertuples():
    redshift.append(x.Redshift)

print(redshift)

#Affichage des données brutes : 

magSN2 = np.array(magSN2)
tSN2 = np.array(timeSN2)

subplot(4,1,1)
ylabel('Apparent magnitude')
  
subplot(4,1,2)
ylabel('Error on magnitude')

xlabel('Time in Supernova referential frame')

subplot(4,1,3)
ylabel('Magnitude + error')

xlabel('Time in referential frame')

subplot(4,1,4)
xlabel('Supernova number')
ylabel('Redshift')



for i in range(100,151):
  subplot(2,1,1)
  plot(tSN2[i][3:], -np.array(magSN2[i][3:]))

z = np.loadtxt('Data-LightCurves/EI2019-Data-LightCurves-SN-Redshifts.txt')
subplot(2,1,2)
plot(z, 'ro')

print(redshiftSN2[0] )

#Approximation avec scipy

import matplotlib.pyplot as plt
from scipy.optimize import curve_fit

result_optSN2=200*[0]
result_min=[]
error = []

j = 200 #Number of supernovas to fit

def fun_to_fit(x, par0, par1, par2):
  return (par0 - lc(x-par2, par1, redshiftSN2[i]) ) #Par0 : distance modulus, par1 : scale factor, par2 : time at maximum

for i in range(50, j):
  result_optSN2[i]= (curve_fit(fun_to_fit, tSN2[i][3:], -np.array(magSN2[i][3:])))
  
  print("Supernova n°{}".format(i))
  print("Parameters value (distance modulus, scale factor, time at maximum) : ")
  print(result_optSN2[i][0])
  print("Covariance matrix with parameters in the same order : ")
  print(result_optSN2[i][1])

for i in range(50, 200 ):
  if i == 138 or i == 68 or i == 94 or i == 106 or i == 120 or i == 74:
    continue
  t_corrSN2 = (tSN2[i][3:]-result_optSN2[i][0][2])/((redshiftSN2[i]+1)*(result_optSN2[i][0][1])) #Time corrected for redshift, scale and time at maximum effects of the supernovae
  mag_corrSN2 = -np.array(magSN2[i][3:])-result_optSN2[i][0][0] - 1.52*(result_optSN2[i][0][1]-1) #Magnitude corrected for redshift, scale and time at maximum effects of the supernovae

  #We inverse here the relation to plot the apparent curves on the absolute mean curve

  plot(t_corrSN2 , mag_corrSN2, 'o')

  plt.xlabel('Time in our reference frame (in days)')
  plt.ylabel('- Absolute magnitude')

  #We also plot the curve of the average magnitude

  plot(tSN2[i][1:], -lc(tSN2[i], 1)[1:], 'b' , linewidth = 1.9)

"""Ces résultats semblent concluants.

L'on ne dispose pas des incertitudes réelles sur les résultats et l'estimation des constantes cosmologiques est, par conséquent, compliquée.
"""